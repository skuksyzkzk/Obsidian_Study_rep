
오프라인 국제어 수업 


![[0-2. Inntroduciton to ML.pdf]]

# 1. what is machine learning ?

> 그저 하나의 정의 일 뿐 노말한 매번 정의는 바뀐다

performance of programs 는 뭐지? 
=> 이건 계속 변한다 may change depend on me (my anything )

accuracity 는 계속 변한다 정확성은 사람마다 

## data,previous results , experiences 

이 3가지는 결국 data 기반 
> data 기반으로 한 프로그램의 성능을 향상시키는 기술
> 

data를 당연히 분석할줄알아야된다 data 기반이니까 

그러면 data-analysis 기술인건가? 

not perfect 

> using data -analysis techniques to improve the performance of programs 

위에가 정의

왜 그러면 머신러닝이 필요할까 통계학과 뭐가 달라?

> 통계학은 수학의 한 줄기 ML은 아니다
> 말같지도 않은 수학적이지 않은 가정을 추가할수있다는것 데이터 분석학에 

만약에 90% cover하고 10% cover 못하면 이건 수학적으로 knowledge가 아니다

> heuristic => 대충어림잡아서 (발견법)

하지만 이것은 heuristic knowledge 이다 
most of case를 cover 하는 것이 heuristic knowledge 

이 논리를 사용해도 괜찮다 

통계학의 관점에서는 사용하지않는 논리지만 
머신러닝은 수학이 아니다 그러므로 우리는 rough하게 사용 할 수 있다.

## we can use ML to many area 

>전부 적용되는 사례들은 **based on data**이다 

### AI vs ML vs D-N-N

AI가 superset이다 

AI is programs with intelligence 
intelligence를 code로 program에 put in 한 것 
ai가 안들어간 프로그램을 보면 구분안감 들어간것하고 왜냐 둘다 code로 이루어져있기에

#### Where to obtain Intelligence 



두 파트로 나뉘는데
1. rule-based ai 
	1. intelligence from human 사람은 캣과 독을 구분할수있는 지능이있다 사람에게서온다
	2. 구분한다고하지만 얼마나 많은 룰을 우리가 작성할수있을까 ? 우리는 분명 구분할수있지만 ... 그렇기에 이것은 impossible하다 human beings는 아는 것에 대해 자세히 말을 하기 힘들어잉
	3. 2000년도까지 가다가 winter가 왔다 
	4. 2010 ~ 2015
2. ai based on ML
	1. intelligence from data 
	2. data도 결국 사람이 만든다
	3. 많은 기술들이 있다 
	4. DNN은 단지 ML의 많은 기술중 하나일뿐 
	5. DNN은 매우 빈번하게 쓰인다
	6. DNN이 다른 ML 기술들에 비해 많이쓰여서 DNN이 superset 이라고 생각하지만 실제론 ML이 superset 

### 결국 AI가 DNN과 ML의 superset



5pg

from data 
- 데이터는 결국 intelligence 행동의 결과이다 
- data와 컴퓨터가 많아지면서 급격하게 성장한것 

6pg

## big data vs machine learning

### big data
이거는 데이터 중심 
어떻게 데이터를 모으고 처리하고 이런것 

### machine learning
데이터로 부터 어떻게 효과적인 패턴을 이끌어낼까?
즉 
> how to analyze data 

두 가지는 some intersection이 존재한다

빅데이도 데이터를 효과적으로 분석하기에 교집합

7pg

머신러닝은 데이터를 다루기에 당연히 통계학과 관련 
테이블과 관련 있고 테이블은 matrix 이기에 linear algebra관련 
analysis 와 최적화는 매우 근접하다 


8~ 다음에 얘기하자 


# 1주차 2번째 수업 

8 pg 

ML - problem solving techniques 
	- a kind of data analysis tech 
	- we want to solve problem using data -analysis tech 

P1 P2 문제들은 각자 그룹화 할수있다 몇몇의 같은 특징으로 

classfication은 문제를 그룹화하는 것 
이런 문제들은 뭐뭐다 이렇게 

어떤 문제들은 regression이라고한다 

ML타입은 매우많다 그중에 일부들을 설명한것 

이러한 문제들을 풀기위해선 각각의 문제에 대해서 많은 ML 기술들을 적용할수있따

> most of problem 은 classfication 과 regression 문제이다 90% ~ 99%

그래서 여기를 다루는 기술이 가장 기술한 기술들이다 

Clustering and dimension reduction => 15~20 % 

labeled data는 x1 has red 이런느낌 
class 는 labeled data들이 given하는 것 

labeled data set이 given 해야 된다 그래야 class 

> [!tip] Labeled data vs Unlabeled data 
> For example, **in a dataset of emails, each email might be labeled as "spam" or "not spam."** These labels then provide a clear guide for a machine learning algorithm to learn from. Suppose we have a facial recognition task. Unlabeled data would consist of a set of facial images without any identification information.




labeled data를 collect 한다 

그 다음 data가 어디에 속할지 결정 

> data is vector 

boundary를 찿아야된다 


## regression

> a set of labeled samples is given
> 

classification에서 label은 categorical value이고 
regression에서 label은 real number 이기에 이거 보고 문제가 무슨 문제인지 구분가능하다 

x는 any dimension 의 vector가 가능하다 

나에게 given data를 가지고 새로운 x`의 real number인 label값을 에측

1차원의 경우에는 양옆의 벡터의 realnumber 값으로 중간값을 내는 등 쉽게 예측이 가능하지만 다차원 100차원 이렇게 가버리면 예측하기가 매우 어려워진다

classfication에서 example 로 cat,dog,cow 를 차례로 0,1,2로 할당한다면
그건 real number 가 된다 

> 둘은 mathmatically하게 equevalent 하다

### 하지만 왜 다르게 구분 할까?

> 각각 충분히 많은 문제들이 있기에 같은 것으로 치부한다면 
> Practical한 이유로 unconvenient하다 

그것도 있고 그냥 classification은 categorical한 값을 구분하는데 사용하고 regression은 가격이나 수입등 continous한 값을 구분하는데 사용하는 듯 practical하게 쓰이는 것이 다르다 

most하게 regression을 해결한 기법은 classfication문제도 해결할수있다


## Clustering

> unlabeled data => just data sample 

분류문제처럼 data들에게 label을 부여 할 필요가없다 

그냥 **Grouping** 하는 것 

#### why group? 

>classfication과 regression은 predition이다

하지만 왜 group할까 

실제로 less important하긴하다 2개에 비해서 

온라인 쇼핑몰에서 무엇이 가장 most important issue일까

1. profit => understand customer 

고객을 이해하고 분석할때 쉽게 하는 법은 그룹핑하는 것이다 

고객의 정보를 다차원의 벡터정보로 업로드하고 유사한 정보를 가진 고객끼리 그룹화 한다

예) 여성,20대,등등등

## dimension reduction

dimension reduction makes informatin loss 
information loss is not good 

>hig dimension data => hard to understand , handle , store 

메모리도 많이 잡아먹고 이해 분석하기 힘듬 

*가장 중요한 것은 minimizing the loss of information*

## Supervised & unsupervised learning

1. Supervised => a group of ml tech **labeled data**
2. unSupervised => a group of ml tech **unlabeled data**

most of case 에서 unS와 S의 기술들이 교차해서 적용했을때 해결되는 경우는 거의없다

UnS tech can sovle S problem 하지만 이건 exception이다 

그냥 단순히 이런게 있다 정도만 이해하자 
> Clear 한 정의 가 아직까진 없기에 








# 0 . First 
![[0-1. Introduction.pdf]]

너무 많은 topic 이 있어서 일부만 다룰 예정 

추가 책은 정말의 필요에 의해서 구매 할 거 아니면 copy해서 제공 

give me the reaction 

machine learing 은 data 와 관련되있고 handle하는 거기에 
linear algebra handle the matrix and matrix is the table and data is in the table so it is important , 

>**additional study => linear algebra** 

deap neural network > 이건 다른 class가 있다 우리가 이걸 지금 cover 해야 될까 ? 
=> 커버하더라도 small part만 cover한다 ( 아직 정해지지 않음)

Hidden markov model => traditioanl 
it is not efficient prepare to deap neural network 
so we do not 

#### Evaluation can be changed without notice

## we can dicuss with my friends for my HWs

but do it by myself 

![[Pasted image 20240304170008.png]]

당장 다음주는 녹화강의 없음 
4월 10일은 녹화강의 있을수있음 

아리까리 한 거 질문 하자 



![[1. k-NN.pdf]]



>K - number of neighbors 

Classfication and Regression are almost same so we can use Knn both

1 near = > red
3 near => blue 

인접한 것의 정보들로 predict 2는 red,blue뜨면 판별안되서 홀수로 

k가 낮을 수록 error는 적지만 boundary가 Complex하고

높을수록 error는 많지만 boundary가 simple하고 smoothly하다

어떤 것이 *better할까?*

### low error and simple boundary are better !

#### Why simple boundary better?????????

> prediction이 쉬워서 ! 

Complex mean sensitive !!!!
- sensitive easy to make error 
- low prediction performance

Simple means Robust !!
- not senstive
- high prediction performance 

But very simple 은 오히려 안 좋다 !! 
=> low prediction performance 이기에 

#### under-fitting 
> k = large nubmer 

boundary very simple => stupid 

#### over-fitting 
> k= 1 

boundary very complex => sensitve 

### 적절하게 generalization하자 


